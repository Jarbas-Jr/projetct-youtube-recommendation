{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "\n",
    "import bs4 as bs4\n",
    "import json\n",
    "\n",
    "import glob\n",
    "import tqdm\n",
    "\n",
    "pd.set_option(\"max.columns\", 131)\n",
    "\n",
    "#https://strftime.org/\n",
    "%matplotlib inline\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anteriormente separamos quais os exemplos que tinham a maior probabilidade de nos trazer ganho caso anotassemos os 'y' deles, mario fez o processo de labeling que nem da outra vez, no google shets, colocando 0 e 1 manualmente, analisando o titulo.\n",
    "\n",
    "Vamos ver o resultado do active learning, como a gente mede o impacto ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(498, 16)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"raw_data_with_labels.csv\", index_col=0)\n",
    "df1 = df1[df1['y'].notnull()]\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 18)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"active_labels1_done.csv\", index_col=0)\n",
    "df2 = df2[df2['y'].notnull()]\n",
    "df2['novo'] = 1\n",
    "df2.shape\n",
    "\n",
    "# 498 feitas inicialmente, e agora df2, as novas com 100 label novas pra gente usar.\n",
    "# adicionamos a coluna novo, vamos entender logo mais abaixo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>watch-title</th>\n",
       "      <th>y</th>\n",
       "      <th>watch-view-count</th>\n",
       "      <th>watch-time-text</th>\n",
       "      <th>content_watch-info-tag-list</th>\n",
       "      <th>watch7-headline</th>\n",
       "      <th>watch7-user-header</th>\n",
       "      <th>watch8-sentiment-actions</th>\n",
       "      <th>og:image</th>\n",
       "      <th>og:image:width</th>\n",
       "      <th>og:image:height</th>\n",
       "      <th>og:description</th>\n",
       "      <th>og:video:width</th>\n",
       "      <th>og:video:height</th>\n",
       "      <th>og:video:tag</th>\n",
       "      <th>channel_link_0</th>\n",
       "      <th>p</th>\n",
       "      <th>novo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>505</td>\n",
       "      <td>Platform Overview - Machine Learning</td>\n",
       "      <td>0</td>\n",
       "      <td>4.298 visualizações</td>\n",
       "      <td>Publicado em 21 de mai. de 2019</td>\n",
       "      <td>Ciência e tecnologia</td>\n",
       "      <td>Platform Overview - Machine Learning</td>\n",
       "      <td>Google Cloud Platform\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>4.298 visualizações\\n\\n\\n\\n\\n\\n\\n\\n141\\n\\nGost...</td>\n",
       "      <td>https://i.ytimg.com/vi/QR_LQQ-vvko/maxresdefau...</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>In this short GCP Essentials video, see how GC...</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>Alexis Moussine Pouchkine</td>\n",
       "      <td>/channel/UCJS9pqu9BzkAMNTmzNMNhvg</td>\n",
       "      <td>0.502</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              watch-title  y     watch-view-count  \\\n",
       "505  Platform Overview - Machine Learning  0  4.298 visualizações   \n",
       "\n",
       "                     watch-time-text content_watch-info-tag-list  \\\n",
       "505  Publicado em 21 de mai. de 2019        Ciência e tecnologia   \n",
       "\n",
       "                          watch7-headline  \\\n",
       "505  Platform Overview - Machine Learning   \n",
       "\n",
       "                                    watch7-user-header  \\\n",
       "505  Google Cloud Platform\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "\n",
       "                              watch8-sentiment-actions  \\\n",
       "505  4.298 visualizações\\n\\n\\n\\n\\n\\n\\n\\n141\\n\\nGost...   \n",
       "\n",
       "                                              og:image  og:image:width  \\\n",
       "505  https://i.ytimg.com/vi/QR_LQQ-vvko/maxresdefau...          1280.0   \n",
       "\n",
       "     og:image:height                                     og:description  \\\n",
       "505            720.0  In this short GCP Essentials video, see how GC...   \n",
       "\n",
       "     og:video:width  og:video:height               og:video:tag  \\\n",
       "505          1280.0            720.0  Alexis Moussine Pouchkine   \n",
       "\n",
       "                        channel_link_0      p  novo  \n",
       "505  /channel/UCJS9pqu9BzkAMNTmzNMNhvg  0.502     1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de avaliar, mario ficou curioso de algo.\n",
    "\n",
    "Se usarmos essas probabilidades que outros modelo nos deu, agora que temos as labels no dataset, quais serão as minhas métricas nesse dataset que acabamos de fazer as labels se usarmos as probabilidades lá do primeiro modelo.\n",
    "\n",
    "O que a gente pode tentar enteder é, ta perto ou longe do que esperavamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2037344613689981, 0.5386250885896527)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nos dados de validação que temos, a average_precision era 0.19, agora 0.20, quer dizer que modelo ta realmente generalizando\n",
    "# aqui o auc era 0.58, aqui ta bem mais baixo, mas ele ta muito sensivel ao numero de exemplos. Realmente esperto uma grande variação\n",
    "# mas eles não estão absurdamente ruins ou diferentes do que esperavamos.\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "average_precision_score(df2['y'], df2['p']), roc_auc_score(df2['y'], df2['p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#juntamos df1 e df2, tirando a coluna p.\n",
    "# quando juntarmos, a gente vair criar a coluna novo, que esta em df2 somente, tbm em df1, so que vai preenhcer com NaN\n",
    "\n",
    "df = pd.concat([df1, df2.drop(\"p\", axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_limpo = pd.DataFrame(index=df.index)\n",
    "df_limpo['title'] = df['watch-title']\n",
    "# agora colocamos a coluna novo, e ao inves de ter os valores nulos, preenchemos com 0\n",
    "# se o exemplo pertencer ao df novo ele vai ter 1, se pertencer aos exemplos que ja tinha as anotações, ele vai ter 0\n",
    "# isso vai ajudar a comparar o efeito do active learning\n",
    "df_limpo['novo'] = df['novo'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte padrão dos outros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_date = df['watch-time-text'].str.extract(r\"(\\d+) de ([a-z]+)\\. de (\\d+)\")\n",
    "clean_date[0] = clean_date[0].map(lambda x: \"0\"+x[0] if len(x) == 1 else x)\n",
    "#clean_date[1] = clean_date[1].map(lambda x: x[0].upper()+x[1:])\n",
    "\n",
    "mapa_meses = {\"jan\": \"Jan\",\n",
    "              \"fev\": \"Feb\",\n",
    "              \"mar\": \"Mar\", \n",
    "              \"abr\": \"Apr\", \n",
    "              \"mai\": \"May\", \n",
    "              \"jun\": \"Jun\",\n",
    "              \"jul\": \"Jul\",\n",
    "              \"ago\": \"Aug\", \n",
    "              \"set\": \"Sep\", \n",
    "              \"out\": \"Oct\", \n",
    "              \"nov\": \"Nov\",\n",
    "              \"dez\": \"Dec\"}\n",
    "\n",
    "clean_date[1] = clean_date[1].map(mapa_meses)\n",
    "\n",
    "clean_date = clean_date.apply(lambda x: \" \".join(x), axis=1)\n",
    "clean_date.head()\n",
    "df_limpo['date'] = pd.to_datetime(clean_date, format=\"%d %b %Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "views = df['watch-view-count'].str.extract(r\"(\\d+\\.?\\d*)\", expand=False).str.replace(\".\", \"\").fillna(0).astype(int)\n",
    "df_limpo['views'] = views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame(index=df_limpo.index)\n",
    "y = df['y'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['tempo_desde_pub'] = (pd.to_datetime(\"2019-12-03\") - df_limpo['date']) / np.timedelta64(1, 'D')\n",
    "features['views'] = df_limpo['views']\n",
    "features['views_por_dia'] = features['views'] / features['tempo_desde_pub']\n",
    "features = features.drop(['tempo_desde_pub'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>views</th>\n",
       "      <th>views_por_dia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>28028</td>\n",
       "      <td>61.464912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1131</td>\n",
       "      <td>2.960733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1816</td>\n",
       "      <td>8.446512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1171</td>\n",
       "      <td>10.455357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1228</td>\n",
       "      <td>3.336957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   views  views_por_dia\n",
       "0  28028      61.464912\n",
       "1   1131       2.960733\n",
       "2   1816       8.446512\n",
       "3   1171      10.455357\n",
       "4   1228       3.336957"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aumenta validação\n",
    "\n",
    "Antes a gente simplesmente dividia entre antes de abril e depois de abril, umas das maneiras que vamos usar pra entender se o active learning ta trazendo algum efeito pra gente é aumentar apenas o dataset de validação.\n",
    "\n",
    "\n",
    "Geralmente preferimos que o set de validação seja o mesmo, mas nesse caso como tinhamos poucos dados de validação, e como to vendo que o auc ta muito instavel, quero ver se só adicionamos os novos exemplos ao dataset de validação, qual vai ser a minha estimativa das métricas, se eu usar os mesmo dados de treino(sem as linhas novas) mas validar nesse dataset aumentado.\n",
    "\n",
    "No active learning, em geral, pensamos em add linhas no treino mas nesse caso estamos precisando de exemplos na validação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((228, 2), (316, 2), (228,), (316,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_train = (df_limpo['date'] < \"2019-04-01\") & (df_limpo['novo'] == 0)\n",
    "\n",
    "mask_val = (df_limpo['date'] >= \"2019-04-01\")\n",
    "\n",
    "Xtrain, Xval = features[mask_train], features[mask_val]\n",
    "ytrain, yval = y[mask_train], y[mask_val]\n",
    "Xtrain.shape, Xval.shape, ytrain.shape, yval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "title_train = df_limpo[mask_train]['title']\n",
    "title_val = df_limpo[mask_val]['title']\n",
    "\n",
    "title_vec = TfidfVectorizer(min_df=2)\n",
    "title_bow_train = title_vec.fit_transform(title_train)\n",
    "title_bow_val = title_vec.transform(title_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228, 193)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_bow_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack, vstack\n",
    "\n",
    "Xtrain_wtitle = hstack([Xtrain, title_bow_train])\n",
    "Xval_wtitle = hstack([Xval, title_bow_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((228, 195), (316, 195))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_wtitle.shape, Xval_wtitle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       n_estimators=1000, n_jobs=6, oob_score=False,\n",
       "                       random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl = RandomForestClassifier(n_estimators=1000, random_state=0, class_weight=\"balanced\", n_jobs=6)\n",
    "mdl.fit(Xtrain_wtitle, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = mdl.predict_proba(Xval_wtitle)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui está, antes tinhamos 0.19 de average_precision e agora 0.1872, apesar de estar melhor mario nao acha que piorou, mas temos poucos dados entao esta em um intervalo de variação.\n",
    "\n",
    "O auc que esta sendo a metrica mais variavel, mais dificil de fazer se confiavel, aumentou um pouco, mas nao consideramos que melhorou mas ta dentro da variação esperada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1872802752830275, 0.5935436218282933)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_precision_score(yval, p), roc_auc_score(yval, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANTES  \n",
    "ap 0.1918043901336543, auc 0.5848024316109421 - mindf=2  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUMENTANDO TREINO\n",
    "\n",
    "Maneira mais tradicional de fazer isso.\n",
    "\n",
    "Dessa vez aumento os dados de treino e mantenho a mesma validação que eu tava usando antes de ter esses novos exemplso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_train = (df_limpo['date'] < \"2019-04-01\")\n",
    "mask_val = (df_limpo['date'] >= \"2019-04-01\") & (df_limpo['novo'] == 0)\n",
    "#mask_val = (df_limpo['date'] >= \"2019-04-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.17697903398635972, 0.5964133738601823)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain, Xval = features[mask_train], features[mask_val]\n",
    "ytrain, yval = y[mask_train], y[mask_val]\n",
    "Xtrain.shape, Xval.shape, ytrain.shape, yval.shape\n",
    "\n",
    "title_train = df_limpo[mask_train]['title']\n",
    "title_val = df_limpo[mask_val]['title']\n",
    "\n",
    "title_vec = TfidfVectorizer(min_df=2)\n",
    "title_bow_train = title_vec.fit_transform(title_train)\n",
    "title_bow_val = title_vec.transform(title_val)\n",
    "\n",
    "Xtrain_wtitle = hstack([Xtrain, title_bow_train])\n",
    "Xval_wtitle = hstack([Xval, title_bow_val])\n",
    "\n",
    "\n",
    "mdl = RandomForestClassifier(n_estimators=1000, random_state=0, class_weight=\"balanced\", n_jobs=6)\n",
    "mdl.fit(Xtrain_wtitle, ytrain)\n",
    "\n",
    "p = mdl.predict_proba(Xval_wtitle)[:, 1]\n",
    "\n",
    "average_precision_score(yval, p), roc_auc_score(yval, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANTES  \n",
    "ap 0.1918043901336543, auc 0.5848024316109421 - mindf=2  \n",
    "\n",
    "Valid nova, treino antigo  \n",
    "ap 0.1872802752830275, auc 0.5935436218282933\n",
    "\n",
    "Valid antiga, treino novo  \n",
    "ap 0.17697903398635972, auc 0.5964133738601823"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agora tudo novo, sem colocar todos os dados novos no treino ou na validação, mas simplesmente dividindo treino e validação em abril de 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_train = (df_limpo['date'] < \"2019-04-01\")\n",
    "#mask_val = (df_limpo['date'] >= \"2019-04-01\") & (df_limpo['novo'] == 0)\n",
    "mask_val = (df_limpo['date'] >= \"2019-04-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.19342838733336654, 0.6095324991310394)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain, Xval = features[mask_train], features[mask_val]\n",
    "ytrain, yval = y[mask_train], y[mask_val]\n",
    "Xtrain.shape, Xval.shape, ytrain.shape, yval.shape\n",
    "\n",
    "title_train = df_limpo[mask_train]['title']\n",
    "title_val = df_limpo[mask_val]['title']\n",
    "\n",
    "title_vec = TfidfVectorizer(min_df=2)\n",
    "title_bow_train = title_vec.fit_transform(title_train)\n",
    "title_bow_val = title_vec.transform(title_val)\n",
    "\n",
    "Xtrain_wtitle = hstack([Xtrain, title_bow_train])\n",
    "Xval_wtitle = hstack([Xval, title_bow_val])\n",
    "\n",
    "\n",
    "mdl = RandomForestClassifier(n_estimators=1000, random_state=0, class_weight=\"balanced\", n_jobs=6)\n",
    "mdl.fit(Xtrain_wtitle, ytrain)\n",
    "\n",
    "p = mdl.predict_proba(Xval_wtitle)[:, 1]\n",
    "\n",
    "average_precision_score(yval, p), roc_auc_score(yval, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANTES  \n",
    "ap 0.1918043901336543, auc 0.5848024316109421 - mindf=2  \n",
    "\n",
    "Valid nova, treino antigo  \n",
    "ap 0.1872802752830275, auc 0.5935436218282933\n",
    "\n",
    "Valid antiga, treino novo  \n",
    "ap 0.17697903398635972, auc 0.5964133738601823\n",
    "\n",
    "Tudo novo  \n",
    "ap 0.19342838733336654, auc 0.6095324991310394"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mario acha que isso ta dentro do esperado.\n",
    "\n",
    "\n",
    "Claro que gostamos de ver mais subindo as métricas, mas nesse caso nao tem como comparamos muito com os valores antigos, pq mudamos tanto os dados de treino como de validação, e a gente nao pode realmente falar que o modelo ta muito melhor que o anterior.\n",
    "\n",
    "Essa nao e a unica maneira de fazer o active learning, tem outras estrategias que a gente pode usar, queria te mostrar esse processo que faz muito sentido e mario ja viu funcionar muito bem em varios casos.\n",
    "\n",
    "Não em dados tao pequenos, mas em casos com dataset de 6mil exemplos, e depois a empresa mandou pra algum serviço de anotação aqueles exemplos que pareciam confudir mais o modelo, foram aumentando aos poucos o dataset e a gente conseguiu otimizar o orçamento pra fazer essas anotações.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### obs: como a gente ta precisando muito de dados e temos apenas cerca de 1100~1200 exemplos, mario vai fazer as labels pra todos manualmente no google sheets,  pra que possamos realmente criar um modelo mais legal.\n",
    "\n",
    "Em regra se a gente tiver mais dados tanto pra treinar como pra validar é melhor, claro que existem exceções como em time series, que as vezes temos muitos dados do passado e nao funciona tao bem quanto ter so de mais recentes. Mas se tiver que escolher entre ter mais dados e ter menos dados é melhor ter mais e ver que eles nao tao ajudando do que não ter mais dados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "\n",
    "import bs4 as bs4\n",
    "import json\n",
    "\n",
    "import glob\n",
    "import tqdm\n",
    "\n",
    "pd.set_option(\"max.columns\", 131)\n",
    "\n",
    "#https://strftime.org/\n",
    "%matplotlib inline\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHEGAMOS A PARTE DA MODELAGEM.\n",
    "\n",
    "Parte que a gente aprende tanto a fazer e vamos ver como perto das outras partes e pequena e relativamente simples de fazer.\n",
    "\n",
    "Agora vou ter esse novo dataset, onde juntei todos as linhas que a gente ja tinha com anotações e tbm as linhas que nao tinham anotações, e no fim mario fez anotação pra todas as 1100~1200 linhas.\n",
    "\n",
    "Anteriormente ele nao havia preenchido todas so para mostrar o processo de active learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"labels_curso - to_label_2.csv\",index_col=0).dropna(subset=['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated(['watch-title']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1164, 16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_limpo = pd.DataFrame(index=df.index)\n",
    "df_limpo['title'] = df['watch-title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIS UMAS VEZ O MESMO PROCESSO DE LIMPEZA DE DADOS, VIEWS E FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpeza dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_date = df['watch-time-text'].str.extract(r\"(\\d+) de ([a-z]+)\\. de (\\d+)\")\n",
    "clean_date[0] = clean_date[0].map(lambda x: \"0\"+x[0] if len(x) == 1 else x)\n",
    "#clean_date[1] = clean_date[1].map(lambda x: x[0].upper()+x[1:])\n",
    "\n",
    "mapa_meses = {\"jan\": \"Jan\",\n",
    "              \"fev\": \"Feb\",\n",
    "              \"mar\": \"Mar\", \n",
    "              \"abr\": \"Apr\", \n",
    "              \"mai\": \"May\", \n",
    "              \"jun\": \"Jun\",\n",
    "              \"jul\": \"Jul\",\n",
    "              \"ago\": \"Aug\", \n",
    "              \"set\": \"Sep\", \n",
    "              \"out\": \"Oct\", \n",
    "              \"nov\": \"Nov\",\n",
    "              \"dez\": \"Dec\"}\n",
    "\n",
    "clean_date[1] = clean_date[1].map(mapa_meses)\n",
    "\n",
    "clean_date = clean_date.apply(lambda x: \" \".join(x), axis=1)\n",
    "clean_date.head()\n",
    "df_limpo['date'] = pd.to_datetime(clean_date, format=\"%d %b %Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpeza de Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "views = df['watch-view-count'].str.extract(r\"(\\d+\\.?\\d*)\", expand=False).str.replace(\".\", \"\").fillna(0).astype(int)\n",
    "df_limpo['views'] = views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame(index=df_limpo.index)\n",
    "y = df['y'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['tempo_desde_pub'] = (pd.to_datetime(\"2019-12-03\") - df_limpo['date']) / np.timedelta64(1, 'D')\n",
    "features['views'] = df_limpo['views']\n",
    "features['views_por_dia'] = features['views'] / features['tempo_desde_pub']\n",
    "features = features.drop(['tempo_desde_pub'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>views</th>\n",
       "      <th>views_por_dia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>28028</td>\n",
       "      <td>61.464912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>394</td>\n",
       "      <td>1161</td>\n",
       "      <td>21.109091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>393</td>\n",
       "      <td>141646</td>\n",
       "      <td>809.405714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>392</td>\n",
       "      <td>325</td>\n",
       "      <td>21.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>391</td>\n",
       "      <td>61</td>\n",
       "      <td>7.625000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      views  views_por_dia\n",
       "0     28028      61.464912\n",
       "394    1161      21.109091\n",
       "393  141646     809.405714\n",
       "392     325      21.666667\n",
       "391      61       7.625000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((555, 2), (609, 2), (555,), (609,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 555 para treino e 609 pra validação, ainda é um dataset muito pequeno\n",
    "\n",
    "mask_train = df_limpo['date'] < \"2019-04-01\"\n",
    "mask_val = (df_limpo['date'] >= \"2019-04-01\")\n",
    "\n",
    "Xtrain, Xval = features[mask_train], features[mask_val]\n",
    "ytrain, yval = y[mask_train], y[mask_val]\n",
    "Xtrain.shape, Xval.shape, ytrain.shape, yval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "title_train = df_limpo[mask_train]['title']\n",
    "title_val = df_limpo[mask_val]['title']\n",
    "\n",
    "title_vec = TfidfVectorizer(min_df=2, ngram_range=(1,4))\n",
    "title_bow_train = title_vec.fit_transform(title_train)\n",
    "title_bow_val = title_vec.transform(title_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(555, 1333)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_bow_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack, vstack\n",
    "\n",
    "Xtrain_wtitle = hstack([Xtrain, title_bow_train])\n",
    "Xval_wtitle = hstack([Xval, title_bow_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((555, 1335), (609, 1335))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_wtitle.shape, Xval_wtitle.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                       n_jobs=6, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tirando a quantidade de árvores, que é o n_estimators, que normalmente devemos tentar colocar o maximo possivel que a memoria do PC aguenta\n",
    "# outro parametro importante é o min_samples_leaf, que determina quantos exemplos precisamos pra criar um novo nó.\n",
    "# só vou tentar fazer essa divisão(expandir a árvore) se eu tiver pelo menos um exemplo pra colcar no proximo nó(se for =1).\n",
    "# limitamos a complexidade do modelo definindo parâmetros e assim evitar que o modelo overfite.\n",
    "\n",
    "mdl = RandomForestClassifier(n_estimators=1000, random_state=0, min_samples_leaf=1, class_weight=\"balanced\", n_jobs=6)\n",
    "mdl.fit(Xtrain_wtitle, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = mdl.predict_proba(Xval_wtitle)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21042440914051194"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_precision_score(yval, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6897446482278705"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(yval, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Para árvores 1000, devemos colocar o maximo de árvores que a memória do PC aguentar. Outro parâmetro importante é o 'min_samples_leaf', ele determina quantos exemplos precisamos para criar um novo nó. Então quando a árvores está fazendo a divisão dos dados pra tentar criar um novo nó, ele fala: só vou tentar fazer a divisão(expandir essa árvore) se eu tiver pelo menos um exemplo pra colocar no próximo nó(se for =1).  Essa é uma forma de regularização, de limitarmos a complexidade do modelo e não overfite.\n",
    "\n",
    "ap 0.1887361518270232, auc 0.6766232234475438 - min_samples_leaf = 1\n",
    "\n",
    "##### Ap e auc cairam, mas queremos ver os dois subindo juntos. Como nossos dados são pequenos, mesmo criando nós com com 1 exemplo, os nós acabam ajudando quando criamos esse ensemble, talvez pelo fato de nao termos exemplos o bastante para ter uma redundancia  que nao seja preditiva. O nosso modelo não ta necessariamente memorizando os dados.\n",
    "\n",
    "ap 0.1773056089416357, auc 0.66359025771068 - min_samples_leaf=2\n",
    "\n",
    "#### Agora colocando 100 árvores\n",
    "\n",
    "ap 0.1834073532858915, auc 0.6716990033614437 - n_estimators=100, min_samples_leaf = 1\n",
    "\n",
    "ap e auc cairam novamente, e as arvores precisam ser o mais profundo possivel, e colocando o maximo de árvores para que se faça a média.\n",
    "\n",
    "ap 0.1762251704396226, auc 0.6700772542312909 - n_estimators=100, min_samples_leaf=2 \n",
    "\n",
    "\n",
    "### O que podemos fazer então para melhorar esse modelo ? Mexer no TF-IDF vectorizer, se reduzrimos pra 1, sera que melhora, ou seja se a palavra aparece uma unica vez ela ja ganha uma coluna. Fomos para 1450 palavras, e ai aumentamos bastante o numero de colunas.\n",
    "\n",
    "ap caiu em relação ao valor anterior com esses parametros da random forest, limitar numero de colunas está ajudando o modelo.\n",
    "\n",
    "ap 0.17595568029276232, auc 0.6454708969747007 - min_df=1  (estimators = 1000, min_samples_leaf = 1)\n",
    "\n",
    "vamos testar agora min_df=3, e ele ja começa a perder informação, então vamos ficar no ponto do min_df=2\n",
    "\n",
    "ap 0.17508239417116303, auc 0.650247685321696 - min_df=3  \n",
    "\n",
    "\n",
    "Mas tem outro parâmetros interessante de tunar no tf-idf que é o ngram_range, que é o numero de palavras em sequencia que ele vai pegar pra criar bag of words.\n",
    "\n",
    "##### Então o que ele ta fazendo hoje ?\n",
    "\n",
    "Por exemplo, se eu tiver um vídeo chamado --> 'intro to machine learning', ele vai criar uma coluna para into, to, machine e learning.\n",
    "\n",
    "Quando passo o ngram_range, qual é o menor ngrama que se quer e o maior ? Se eu passo (1,2), entao preciso fazer colunas tanto para palavras individuais, mas também para palavras que aparecem juntas. Ou seja, vamos fazer colunas indicando palavras juntas e  pares de palavras, desde que sejam uma depois da outra.\n",
    "\n",
    "Mario realta que houve casos de (1,8), ou seja, coluna para sequencia de 8 palavras.\n",
    "\n",
    "Vamos testar, tanto o numero de palavras maximas em sequencia, quanto o minimo, nem sempre o numero minimo 1 nao e tao bom.\n",
    "\n",
    "#### Exemplo lúdico de disposição de colunas com base no ngram_range.\n",
    "\n",
    "intro to machine learning -> intro, to, machine, learning -> ngram_range(1,1)\n",
    "\n",
    "intro to machine learning -> intro, to, machine, learning, intro to, to machine, machine learning -> ngram_range(1,2) \n",
    "\n",
    "intro to machine learning -> intro to, to machine, machine learning -> ngram_range(2,2) *apenas colunas com 2 palavras\n",
    "\n",
    "*é interessante da uma retunada no min_df depois que definiu o numero de ngramas, pois ele muda, já que agora o contexto das colunas é outro.\n",
    "\n",
    "ap 0.21140611241061869, auc 0.6785398360559061 - min_df=2, ngram_range=(1,2)  \n",
    "ap 0.22228951304206077, auc 0.6914990859232175 - min_df=2, ngram_range=(1,3) * melhor situação que conseguimos aqui \n",
    "ap 0.20564709615419985, auc 0.6848794008374124 - min_df=2, ngram_range=(1,4)  *aqui ela ja começou a cair\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM - MODELO FAVORITO DO MARIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geralmente bate a random forest, mas tem que tunar mais parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight='balanced',\n",
       "               colsample_bytree=1.0, importance_type='split', learning_rate=0.1,\n",
       "               max_depth=-1, min_child_samples=20, min_child_weight=0.001,\n",
       "               min_split_gain=0.0, n_estimators=100, n_jobs=6, num_leaves=31,\n",
       "               objective=None, random_state=0, reg_alpha=0.0, reg_lambda=0.0,\n",
       "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
       "               subsample_freq=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl = LGBMClassifier(random_state=0, class_weight=\"balanced\", n_jobs=6)\n",
    "mdl.fit(Xtrain_wtitle, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    }
   ],
   "source": [
    "p = mdl.predict_proba(Xval_wtitle)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.19802541702844073, 0.6238131745002063)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_precision_score(yval, p), roc_auc_score(yval, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization\n",
    "\n",
    "Vamos otimizar o LightGBM.\n",
    "\n",
    "Mario NÃO GOSTA DE USAR GRIDSEARCH, nem o modelo Nayve Baises(muito fraco segundo mario).\n",
    "\n",
    "Grid Search demora muito, busca exaustiva.\n",
    "\n",
    "\n",
    "ALTERNATIVAS: Random Search, busca aleatória, selecionamos aleatoriamente algumas combinçãoes dentro de um intervalo razoavel e testam elas. Papers, artigos e etc dizem que random search e mais eficiente que grid search. Trazem resultado melhor em bem menos tempo que GS.\n",
    "\n",
    "Existem algumas técnicas que conseguem ser iguais ou melhores que random search, como o BAYESIAN OPTMIZATION.\n",
    "\n",
    "Nesse caso, o mecanismo que o algortimo de busca vai fazer é: primeiro faz uma busca aleatória, e a partir de um determinado número de amostras que ele tiver de pares de parâmetros e a métrica de erro, ele vai treinar o modelo para que através dos parametros que a gente ta tentando tunar ele tentar prever qual vai ser a métrica de erro.\n",
    "\n",
    "\n",
    "Em 'n_random_starts = 20', a gente vai pegar 20 combinações de parametros aleatoriamente, e com esses 20 a gente vai criar um modelo que vai ser um enseble de árvores, e ai a gente vai gerar 10 mil combinçãoes e vai mandar esse modelo prever qual a combinação que pode me dar o maior ganho, e ele vai testar essa com a proxima combinação na nossa busca.\n",
    "\n",
    "\n",
    "É IMPORTANTE, pq tunar modelos leva um tempo razóavel, imagine que o modelo leve 30 min pra rodar, 3 ou 4h, nao da pra testar muitas combinações dependendo do tamanho do dataset. Entao temos que ter uma busca mais direcionada, e combinações que tem maior chance de dar maior ganho.\n",
    "\n",
    "Ainda não há resultado claro que a bayesian optimazation seja melhor que a random search, mas segundo Mario, em 20, 30 passos acaba dando uma solução boa e ele prefere usar BO.\n",
    "\n",
    "\n",
    "Os parÂmetros ta TF-IDF tbm podem ser tunados, e ele vai tentar tunar tudo aqui junto, nao só do LightGBM, mas do TF-IDF tbm.\n",
    "\n",
    "Antigamente era mais dificil fazer esse tipo de coisa, mas conforme essas buscas ficam mais eficientes, acaba valendo a pena tentar achar todos os parÂmetros.\n",
    "\n",
    "\n",
    "Inclusive, alguns dos melhores resultado com deep learning, são redes neurais com suas arquiteturas encontradas através de buscas mais inteligentes que a aleatória.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import forest_minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "[0.009944912110647982, 5, 1, 0.4677107511929402, 0.49263223036174764, 272, 3, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6016099545910243\n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 0.5845\n",
      "Function value obtained: -0.1404\n",
      "Current minimum: -0.1404\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "[0.053887464791860025, 1, 15, 0.7437489153990157, 0.8675167974293533, 549, 3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5507312614259597\n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 0.3205\n",
      "Function value obtained: -0.1203\n",
      "Current minimum: -0.1404\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "[0.004151454520895999, 6, 20, 0.8682075103820793, 0.9491436163200662, 411, 4, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.587884059680368\n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 0.7169\n",
      "Function value obtained: -0.1487\n",
      "Current minimum: -0.1487\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "[0.0014099928811969545, 9, 9, 0.6502182010234373, 0.6866210554187129, 828, 5, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6245355900218199\n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 2.4828\n",
      "Function value obtained: -0.1555\n",
      "Current minimum: -0.1555\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "[0.08530558241838007, 8, 19, 0.2137736299768322, 0.1313765544201984, 961, 4, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 0.9186\n",
      "Function value obtained: -0.1018\n",
      "Current minimum: -0.1555\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "[0.003567949451535685, 10, 19, 0.7232951768944309, 0.7298538828427115, 939, 4, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5755882526390281\n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 1.7032\n",
      "Function value obtained: -0.1388\n",
      "Current minimum: -0.1555\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "[0.014828577273549474, 7, 1, 0.18428087097824575, 0.3261556557915816, 274, 1, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.585421949637318\n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 1.7219\n",
      "Function value obtained: -0.1444\n",
      "Current minimum: -0.1555\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "[0.0015212976972079912, 3, 12, 0.44234694306528044, 0.399351303640462, 272, 3, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5672288730317863\n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 0.3725\n",
      "Function value obtained: -0.1394\n",
      "Current minimum: -0.1555\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "[0.01946212855369041, 9, 18, 0.5235636153223084, 0.6728679300083596, 747, 4, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5670372117709501\n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 1.2608\n",
      "Function value obtained: -0.1260\n",
      "Current minimum: -0.1555\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "[0.0012116790683302117, 3, 2, 0.06616307483844217, 0.23025600705315752, 677, 2, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5789644394645279\n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 0.6359\n",
      "Function value obtained: -0.1356\n",
      "Current minimum: -0.1555\n",
      "Iteration No: 11 started. Evaluating function at random point.\n",
      "[0.0053139776214487944, 6, 9, 0.14251441334450304, 0.8175761405215897, 297, 1, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5532081146429204\n",
      "Iteration No: 11 ended. Evaluation done at random point.\n",
      "Time taken: 0.3964\n",
      "Function value obtained: -0.1155\n",
      "Current minimum: -0.1555\n",
      "Iteration No: 12 started. Evaluating function at random point.\n",
      "[0.0068572961982704935, 10, 5, 0.2390386584472456, 0.49053406102209746, 176, 2, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5947986082443828\n",
      "Iteration No: 12 ended. Evaluation done at random point.\n",
      "Time taken: 0.4922\n",
      "Function value obtained: -0.1593\n",
      "Current minimum: -0.1593\n",
      "Iteration No: 13 started. Evaluating function at random point.\n",
      "[0.00781968225875022, 3, 4, 0.7078936710077383, 0.31818755505678337, 275, 4, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n",
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6085392463289496\n",
      "Iteration No: 13 ended. Evaluation done at random point.\n",
      "Time taken: 0.4445\n",
      "Function value obtained: -0.1478\n",
      "Current minimum: -0.1593\n",
      "Iteration No: 14 started. Evaluating function at random point.\n",
      "[0.017293945600511968, 2, 15, 0.9007557574888567, 0.41026441194439994, 316, 5, 1]\n",
      "0.5622014507283127\n",
      "Iteration No: 14 ended. Evaluation done at random point.\n",
      "Time taken: 0.1728\n",
      "Function value obtained: -0.1563\n",
      "Current minimum: -0.1593\n",
      "Iteration No: 15 started. Evaluating function at random point.\n",
      "[0.012250750764764855, 8, 6, 0.5976582413192033, 0.2474882432951916, 516, 4, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n",
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6094828094592204\n",
      "Iteration No: 15 ended. Evaluation done at random point.\n",
      "Time taken: 0.8385\n",
      "Function value obtained: -0.1570\n",
      "Current minimum: -0.1593\n",
      "Iteration No: 16 started. Evaluating function at random point.\n",
      "[0.018353598126553926, 4, 3, 0.47305622526323254, 0.1404164811277527, 133, 4, 1]\n",
      "0.6056495842424958\n",
      "Iteration No: 16 ended. Evaluation done at random point.\n",
      "Time taken: 0.1543\n",
      "Function value obtained: -0.1442\n",
      "Current minimum: -0.1593\n",
      "Iteration No: 17 started. Evaluating function at random point.\n",
      "[0.0010383234748454694, 9, 19, 0.9256771571832196, 0.9321438677645206, 312, 4, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5769741109866131\n",
      "Iteration No: 17 ended. Evaluation done at random point.\n",
      "Time taken: 0.6450\n",
      "Function value obtained: -0.1442\n",
      "Current minimum: -0.1593\n",
      "Iteration No: 18 started. Evaluating function at random point.\n",
      "[0.004955229758078229, 5, 5, 0.06939551310802591, 0.4193273080472823, 725, 4, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5457775549920386\n",
      "Iteration No: 18 ended. Evaluation done at random point.\n",
      "Time taken: 0.4223\n",
      "Function value obtained: -0.1091\n",
      "Current minimum: -0.1593\n",
      "Iteration No: 19 started. Evaluating function at random point.\n",
      "[0.0699516121742407, 9, 10, 0.6477856515609233, 0.8594430701440198, 616, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5662558235536947\n",
      "Iteration No: 19 ended. Evaluation done at random point.\n",
      "Time taken: 1.2212\n",
      "Function value obtained: -0.1169\n",
      "Current minimum: -0.1593\n",
      "Iteration No: 20 started. Evaluating function at random point.\n",
      "[0.0014752743467850462, 5, 4, 0.9747950537021096, 0.982207187458162, 909, 2, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5634841068585246\n",
      "Iteration No: 20 ended. Evaluation done at random point.\n",
      "Time taken: 2.4219\n",
      "Function value obtained: -0.1357\n",
      "Current minimum: -0.1593\n",
      "Iteration No: 21 started. Searching for the next optimal point.\n",
      "[0.010039791164582725, 9, 11, 0.6596823900439108, 0.18671284566187135, 467, 4, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5734210060741877\n",
      "Iteration No: 21 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.9512\n",
      "Function value obtained: -0.1295\n",
      "Current minimum: -0.1593\n",
      "Iteration No: 22 started. Searching for the next optimal point.\n",
      "[0.0027864871304710045, 10, 2, 0.07791079181091931, 0.43600928176626413, 321, 2, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5840066049419118\n",
      "Iteration No: 22 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.2262\n",
      "Function value obtained: -0.1501\n",
      "Current minimum: -0.1593\n",
      "Iteration No: 23 started. Searching for the next optimal point.\n",
      "[0.06619381737473454, 10, 6, 0.44652512546757567, 0.41805106564623606, 182, 3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5749248098130565\n",
      "Iteration No: 23 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.8153\n",
      "Function value obtained: -0.1360\n",
      "Current minimum: -0.1593\n",
      "Iteration No: 24 started. Searching for the next optimal point.\n",
      "[0.005553759621868936, 8, 5, 0.28639722115326405, 0.5253328438063181, 545, 5, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5883263548976824\n",
      "Iteration No: 24 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.2534\n",
      "Function value obtained: -0.1373\n",
      "Current minimum: -0.1593\n",
      "Iteration No: 25 started. Searching for the next optimal point.\n",
      "[0.03915771474743115, 10, 4, 0.8745284933940919, 0.609455271676343, 317, 2, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6329539423247036\n",
      "Iteration No: 25 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.6811\n",
      "Function value obtained: -0.1732\n",
      "Current minimum: -0.1732\n",
      "Iteration No: 26 started. Searching for the next optimal point.\n",
      "[0.051594007805739604, 10, 7, 0.8482581530088819, 0.546993868790632, 271, 1, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5723742407265435\n",
      "Iteration No: 26 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.1564\n",
      "Function value obtained: -0.1226\n",
      "Current minimum: -0.1732\n",
      "Iteration No: 27 started. Searching for the next optimal point.\n",
      "[0.06301493913876255, 9, 12, 0.8793393161309506, 0.5415154595335046, 354, 2, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6143185705018577\n",
      "Iteration No: 27 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.1023\n",
      "Function value obtained: -0.1822\n",
      "Current minimum: -0.1822\n",
      "Iteration No: 28 started. Searching for the next optimal point.\n",
      "[0.025670207809790957, 9, 12, 0.9590961508588095, 0.08714971326599637, 345, 2, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6340449371940791\n",
      "Iteration No: 28 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.8767\n",
      "Function value obtained: -0.1694\n",
      "Current minimum: -0.1822\n",
      "Iteration No: 29 started. Searching for the next optimal point.\n",
      "[0.08947810585132653, 10, 17, 0.8597011370253026, 0.19795847958523116, 418, 1, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5857462994633484\n",
      "Iteration No: 29 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.1544\n",
      "Function value obtained: -0.1341\n",
      "Current minimum: -0.1822\n",
      "Iteration No: 30 started. Searching for the next optimal point.\n",
      "[0.057178386105103396, 10, 14, 0.8772743749758142, 0.5707545808135022, 213, 2, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6109276405024473\n",
      "Iteration No: 30 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.9467\n",
      "Function value obtained: -0.1685\n",
      "Current minimum: -0.1822\n",
      "Iteration No: 31 started. Searching for the next optimal point.\n",
      "[0.08935049424620435, 10, 11, 0.9228414973696509, 0.4992991894823078, 115, 2, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6397947750191662\n",
      "Iteration No: 31 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.8380\n",
      "Function value obtained: -0.1793\n",
      "Current minimum: -0.1822\n",
      "Iteration No: 32 started. Searching for the next optimal point.\n",
      "[0.09696964106847213, 9, 11, 0.8844839452157665, 0.15179031797507975, 522, 2, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6117237718936133\n",
      "Iteration No: 32 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.2471\n",
      "Function value obtained: -0.1693\n",
      "Current minimum: -0.1822\n",
      "Iteration No: 33 started. Searching for the next optimal point.\n",
      "[0.06792396653884321, 9, 7, 0.9492958295647879, 0.26326143728995494, 548, 2, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6359910361502625\n",
      "Iteration No: 33 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.5102\n",
      "Function value obtained: -0.1841\n",
      "Current minimum: -0.1841\n",
      "Iteration No: 34 started. Searching for the next optimal point.\n",
      "[0.06541663789826244, 9, 5, 0.9403090579268377, 0.2599634417141055, 649, 2, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.636197440585009\n",
      "Iteration No: 34 ended. Search finished for the next optimal point.\n",
      "Time taken: 2.2883\n",
      "Function value obtained: -0.1739\n",
      "Current minimum: -0.1841\n",
      "Iteration No: 35 started. Searching for the next optimal point.\n",
      "[0.07739399592421241, 7, 4, 0.951939801117928, 0.7388973312601077, 459, 2, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6567494250162175\n",
      "Iteration No: 35 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.3481\n",
      "Function value obtained: -0.1666\n",
      "Current minimum: -0.1841\n",
      "Iteration No: 36 started. Searching for the next optimal point.\n",
      "[0.03674104387174218, 9, 14, 0.9006417514600535, 0.1878246674936721, 937, 2, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5986760629828389\n",
      "Iteration No: 36 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.7366\n",
      "Function value obtained: -0.1308\n",
      "Current minimum: -0.1841\n",
      "Iteration No: 37 started. Searching for the next optimal point.\n",
      "[0.06949429312990789, 8, 3, 0.9126835754696586, 0.28961086840957667, 747, 2, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6657132747537889\n",
      "Iteration No: 37 ended. Search finished for the next optimal point.\n",
      "Time taken: 2.3130\n",
      "Function value obtained: -0.2206\n",
      "Current minimum: -0.2206\n",
      "Iteration No: 38 started. Searching for the next optimal point.\n",
      "[0.06440926762537982, 6, 5, 0.9133600121086718, 0.46308936621869917, 851, 1, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.554490770773132\n",
      "Iteration No: 38 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.7263\n",
      "Function value obtained: -0.1148\n",
      "Current minimum: -0.2206\n",
      "Iteration No: 39 started. Searching for the next optimal point.\n",
      "[0.07638183016628648, 2, 3, 0.9956258846934342, 0.24672482296735465, 612, 2, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6581647697116235\n",
      "Iteration No: 39 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.7811\n",
      "Function value obtained: -0.1876\n",
      "Current minimum: -0.2206\n",
      "Iteration No: 40 started. Searching for the next optimal point.\n",
      "[0.0894811429274582, 7, 1, 0.8563476470190547, 0.07065078561194435, 947, 2, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.688683139706316\n",
      "Iteration No: 40 ended. Search finished for the next optimal point.\n",
      "Time taken: 2.5800\n",
      "Function value obtained: -0.1988\n",
      "Current minimum: -0.2206\n",
      "Iteration No: 41 started. Searching for the next optimal point.\n",
      "[0.07720974927372037, 6, 3, 0.9197621539898518, 0.6449917451643387, 964, 2, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6638261484932476\n",
      "Iteration No: 41 ended. Search finished for the next optimal point.\n",
      "Time taken: 2.4181\n",
      "Function value obtained: -0.1959\n",
      "Current minimum: -0.2206\n",
      "Iteration No: 42 started. Searching for the next optimal point.\n",
      "[0.075306870015126, 3, 1, 0.7757620213157056, 0.9417420093037455, 948, 2, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6387037801497907\n",
      "Iteration No: 42 ended. Search finished for the next optimal point.\n",
      "Time taken: 2.0792\n",
      "Function value obtained: -0.1834\n",
      "Current minimum: -0.2206\n",
      "Iteration No: 43 started. Searching for the next optimal point.\n",
      "[0.08265121231498246, 7, 1, 0.7251351011494334, 0.07547006552546137, 839, 2, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6883293035324645\n",
      "Iteration No: 43 ended. Search finished for the next optimal point.\n",
      "Time taken: 2.2984\n",
      "Function value obtained: -0.2378\n",
      "Current minimum: -0.2378\n",
      "Iteration No: 44 started. Searching for the next optimal point.\n",
      "[0.09833170318391853, 1, 1, 0.754182980619871, 0.08605352068337667, 870, 3, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6422863714100371\n",
      "Iteration No: 44 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.0521\n",
      "Function value obtained: -0.1818\n",
      "Current minimum: -0.2378\n",
      "Iteration No: 45 started. Searching for the next optimal point.\n",
      "[0.07989873876843957, 8, 1, 0.07404579732974367, 0.15481933223031255, 980, 2, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6272630771952585\n",
      "Iteration No: 45 ended. Search finished for the next optimal point.\n",
      "Time taken: 2.0988\n",
      "Function value obtained: -0.1577\n",
      "Current minimum: -0.2378\n",
      "Iteration No: 46 started. Searching for the next optimal point.\n",
      "[0.0410589708939533, 9, 2, 0.9405453517375768, 0.1321498356955883, 911, 2, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6750014743173911\n",
      "Iteration No: 46 ended. Search finished for the next optimal point.\n",
      "Time taken: 4.4019\n",
      "Function value obtained: -0.1792\n",
      "Current minimum: -0.2378\n",
      "Iteration No: 47 started. Searching for the next optimal point.\n",
      "[0.075934313018497, 8, 2, 0.9363697345353915, 0.15129274897268502, 765, 1, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.59317685911423\n",
      "Iteration No: 47 ended. Search finished for the next optimal point.\n",
      "Time taken: 3.6393\n",
      "Function value obtained: -0.1513\n",
      "Current minimum: -0.2378\n",
      "Iteration No: 48 started. Searching for the next optimal point.\n",
      "[0.06655785316110756, 7, 1, 0.7589453100795996, 0.21159340705674562, 770, 2, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6480214660612137\n",
      "Iteration No: 48 ended. Search finished for the next optimal point.\n",
      "Time taken: 2.9721\n",
      "Function value obtained: -0.1582\n",
      "Current minimum: -0.2378\n",
      "Iteration No: 49 started. Searching for the next optimal point.\n",
      "[0.06892466655245144, 7, 5, 0.8261588403741187, 0.07729616979076112, 853, 2, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6603467594503744\n",
      "Iteration No: 49 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.9908\n",
      "Function value obtained: -0.1719\n",
      "Current minimum: -0.2378\n",
      "Iteration No: 50 started. Searching for the next optimal point.\n",
      "[0.06602730762014003, 10, 2, 0.6724229168392452, 0.07445872598097698, 855, 2, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarbasjr/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:546: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6782449725776964\n",
      "Iteration No: 50 ended. Search finished for the next optimal point.\n",
      "Time taken: 4.5583\n",
      "Function value obtained: -0.2132\n",
      "Current minimum: -0.2378\n"
     ]
    }
   ],
   "source": [
    "# No lightGBM Mario gosta de tunar 5 parâmetros, learning rate, profundidade máxima das árvores , numero minimo de exemplos em cada nó\n",
    "# subsample, se vc treina todas as arvores com a mesma amostra, elas tendem a ter mais overfiting(memorizar mais os dados), agora se vcs pega uma amostra aleatoria toda vez que for treinar uma nova árvore, ele tende a regulariza um pouco, reduzindo o overfiting, isso é subsample \n",
    "# colsample_bytree, mesma coisa que sample, só que para coluna, a cada arvore criada, ele seleciona uma parte das colunas para evitar overfiting\n",
    "# vamos tunar tbm o numero de árvores: n_estimators\n",
    "\n",
    "# em RF-IDF vamos tunar min_df e ngram_range(poderia ter colocado um pro minimo tbm, desde que fosse menor que o do maximo).\n",
    "def tune_lgbm(params):\n",
    "    print(params)\n",
    "    lr = params[0]\n",
    "    max_depth = params[1]\n",
    "    min_child_samples = params[2]\n",
    "    subsample = params[3]\n",
    "    colsample_bytree = params[4]\n",
    "    n_estimators = params[5]\n",
    "    \n",
    "    min_df  = params[6]\n",
    "    ngram_range = (1, params[7])\n",
    "    \n",
    "    title_vec = TfidfVectorizer(min_df=min_df, ngram_range=ngram_range)\n",
    "    title_bow_train = title_vec.fit_transform(title_train)\n",
    "    title_bow_val = title_vec.transform(title_val)\n",
    "    \n",
    "    Xtrain_wtitle = hstack([Xtrain, title_bow_train])\n",
    "    Xval_wtitle = hstack([Xval, title_bow_val])\n",
    "    \n",
    "    mdl = LGBMClassifier(learning_rate=lr, num_leaves=2 ** max_depth, max_depth=max_depth, \n",
    "                         min_child_samples=min_child_samples, subsample=subsample, colsample_bytree=colsample_bytree,\n",
    "                         bagging_freq=1, n_estimators=n_estimators, random_state=0, class_weight=\"balanced\", n_jobs=6)\n",
    "    \n",
    "    mdl.fit(Xtrain_wtitle, ytrain)\n",
    "    \n",
    "    p = mdl.predict_proba(Xval_wtitle)[:, 1]\n",
    "    \n",
    "    print(roc_auc_score(yval, p))\n",
    "    \n",
    "    return -average_precision_score(yval, p)\n",
    "\n",
    "\n",
    "space = [(1e-3, 1e-1, 'log-uniform'), #lr, 'log-uniform' pq ao invés de querer que ele pegue de uma distribuição uniforme, ou seja, que cada valor entre esse intervalos tenha a mesma chance de fazer parte de uma combinação, eu quero que ele pegue valores mais próximo da banda inferior, quanto menor o valor da lr, tende a estabilizar o modelo melhor  \n",
    "        # todas as outras seguem uma distribuição uniforme entre a faixa de valores\n",
    "        # se eu ver que o melhor parametro ta no limite de algum desses intervalos(exatamente uma das bandas), ai tento expandir a busca\n",
    "        # mario tem experiencia com lightgbm, entao ele sabe que esses sao os valores mais ou menos\n",
    "        # mas com redes neurais e dificil supor a faixa da arquitetura mais precisa, mesmo para os maiores especialistas\n",
    "        # ai se coloca numeros maiores no intervalo, e ai vai fazendo uma busca cada vez menor\n",
    "        (1, 10), # max_depth\n",
    "        (1, 20), # min_child_samples\n",
    "        (0.05, 1.), # subsamples\n",
    "        (0.05, 1.), # colsample_bytree\n",
    "        (100, 1000), # n_estimators\n",
    "        (1, 5), # min_df\n",
    "        (1, 5)] # ngram_range\n",
    "\n",
    "# vamos rodar isso por 50 iterações e ver o resultado.\n",
    "# na função forest_minimize, o que devemos passar ?\n",
    "# passamos uma função que aceite uma combinação/lista de parametros e retorne um número que é a pontuação que queremos minimizar.\n",
    "# estamos retornando a negativa da average_precision, pq queremos maximizar a métrica, e se tu tem um mecanismo que minimza, então tu coloca um negativo na minimzação, mesma coisa que maximizar\n",
    "# bom fazer print dos parametros e de outra métrica que estamos utilizando pra ter certeza que ta tudo certo, e ver uma combinação de parametros que melhore ambas métricas\n",
    "\n",
    "# além da função de minimização, precisamos passar o 'espaço' de intervalos, lista de tuples, nesse caso colocamos comentado o que cada lista do elemento se refere\n",
    "# os valores que chutamos aprendemos com a experiencia, ou vendo artigos que sugerem\n",
    "res = forest_minimize(tune_lgbm, space, random_state=160745, n_random_starts=20, n_calls=50, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NA ITERAÇÃO 44, batemos ap, mas nao batemos auc.\n",
    "\n",
    "referencia anteior - ap 0.22228951304206077, auc 0.6914990859232175 - min_df=2, ngram_range=(1,3) \n",
    "\n",
    " após tunagem - ap 0.2378, auc 0.6883293035324645 LGBM - [0.08265121231498246, 7, 1, 0.7251351011494334, 0.07547006552546137, 839, 2, 5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por isso que é bom imprimir os parâmetros, pode copia-la, ou usamos 'res.x', nos da a lista de parametros com o melhor score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.08265121231498246, 7, 1, 0.7251351011494334, 0.07547006552546137, 839, 2, 5]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# em ordem: lr, max_depth =7, min_child_samples, subsamble, colsample_bytree, n_estimators, min_df e ngram_range.\n",
    "res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A gente deve usar esse modelo em produção ??????????\n",
    "\n",
    "Antes de definir quais modelos vão pra produção, eu vou testar uma Logistic Regression.\n",
    "\n",
    "Então a gente já sabe que esses parametros do LGBM sao os que funcionam para o melhor score, vamos anotar o da random forest.\n",
    "\n",
    "\n",
    "##### Random Forest: ap 0.22228951304206077, auc 0.6914990859232175 , parametros - min_df=2, ngram_range=(1,3)\n",
    "\n",
    "\n",
    "##### LGBM:  ap 0.2378, auc 0.6883293035324645, parametros -  [0.08265121231498246,  7,  1,  0.7251351011494334,  0.07547006552546137,  839,  2,  5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGRESSÃO LOGÍSTICA\n",
    "\n",
    "\n",
    "Primeiro vamos mostrar como transforma quando se tem colunas esparsas e numericas, e depois ver se a reg log bate os modelos mais complexos que ja testamos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MaxAbsscaler - ele escala os valores dividindo pelo valor maximo absoluto que encontrar em cada coluna. \n",
    "# StandardScaler - faz subtração da media e divide pelo desvio padrao, mas ai perdemos a esparsidade, a estrutura cheia de zeros nessa matriz e nao vale a pena\n",
    "# MaxAbs e recomendado pra isso mas mesmo assim vamos testar ambas normalizações no caso da reg log\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler, StandardScaler\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# como a gente tem colunas esparsas e densas, transformei o Xtrain_wtitle usa os mesmo parametros que usamos no tf-idf da random forest\n",
    "# transformei pra sparsa pra poder usar esses scalers\n",
    "\n",
    "Xtrain_wtitle2 = csr_matrix(Xtrain_wtitle.copy())\n",
    "Xval_wtitle2 = csr_matrix(Xval_wtitle.copy())\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "scaler = MaxAbsScaler()\n",
    "\n",
    "\n",
    "\n",
    "# a primeira coisa que fizemos é fazer o scaling para as variaveis numericas, e aqui pegamos so as colunas 0 e 1, e eu so to escalando elas\n",
    "# só to escalando essas colunas pq nao quero usar nas minhas colunas esparsas pq vai tirar os meu zeros e quebrar a minha estrutura\n",
    "#Xtrain_wtitle2[:, :2] = scaler.fit_transform(Xtrain_wtitle2[:, :2].todense())\n",
    "#Xval_wtitle2[:, :2] = scaler.transform(Xval_wtitle2[:, :2].todense())\n",
    "\n",
    "\n",
    "# Aqui é o scaling com MaxAbs, que pode ser para toda matriz, nao afetando as colunas esparsas\n",
    "Xtrain_wtitle2 = scaler.fit_transform(Xtrain_wtitle2)\n",
    "Xval_wtitle2 = scaler.transform(Xval_wtitle2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(609, 1335)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xval_wtitle2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.5, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=6, penalty='l2', random_state=0,\n",
       "                   solver='lbfgs', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mdl = LogisticRegression(C=0.5, n_jobs=6, random_state=0)\n",
    "mdl.fit(Xtrain_wtitle2, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = mdl.predict_proba(Xval_wtitle2)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2162535946903888, 0.6861178274458927)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_precision_score(yval, p), roc_auc_score(yval, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "primeiro testando sem nenhum tuning, e temos metricas que não estão ruins.\n",
    "##### (0.20616279322296893, 0.6606416229285841) - sem tuning, standardscaler\n",
    "\n",
    "agora trocando o normalizador, nao preciso fazer o scaling so nas duas primeiras colunas, mas em toda matriz.\n",
    "resultado um pouco melhor do que com standard scaler.\n",
    "##### (0.20757989629841797, 0.6862357728371764) - sem tuning, maxabsscaler\n",
    "\n",
    "como quero uma solução rapida, so vou tunar o parametro C, que é a penalidade, coef que multiplica pela penalidade de regularização da reg log. Se ela tivesse dado um resultado muito melhor, talvez nos tunassemos com mais cuidado, mas aqui ja temos dois modelos que são legais. Por padrão é C=1, é uma medida inversa da regularização, quanto maior ele for menor a regularização. Primeior vamos reduzir a regularização e colocar C=10. Resultado muito pior, entao precisamos da regularização proxima de 1 ou até menor que 1.\n",
    "##### (0.18953863996413214, 0.6741463702305833) - C=10, maxabsscaler\n",
    "\n",
    "\n",
    "Aqui no caso, C menor que 1, ja que performou mal com regularização reduzida, mais regularização está nos ajudando\n",
    "##### (0.21340786207179874, 0.6835525151854692) - C=0.5, maxabsscaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A gente poderia continuar tunando pra achar valores melhores e etc. C=0.4, C=0.6.\n",
    "\n",
    "Tem outras coisas que poderiamos tunar mas aqui estamso satisfeitos.\n",
    "\n",
    "\n",
    "Só que, eu poderia escolher um desses modelos pra colocar em produção ou colocar mais de um. Aqui eu nao tenho previsões que precisam ser extremamente rapidas, entao podemos usar um ENSEMBLE pra fazer a previsão final da relevancia de cada video, o rankeamento de cada video. Entaõ vou pegar alguns ou todos esses modelos e vou fazer uma média deles pra ver se conseguimos uma solução ainda melhor, não usando apenas individualmente Random Forest, LGBM ou RegLog, mas uma média ponderada entre eles.\n",
    "\n",
    "Outra vantagem é que teremos uma solução mais robusta, se fizermos a média de modelos que estão errando em lugares diferentes, podemos ganhar perfomance em termos de métrica e ganhar mais estabilidade e robustez na nossa solução."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
